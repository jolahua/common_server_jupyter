{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "In this assignment you will extend the Viterbi algorithm discussed during class.\n",
    "\n",
    "Your mark will depend on \n",
    "\n",
    "+ the **correctness** of your implementation\n",
    "+ the **correctness** and **clarity** of your description of the algorithms\n",
    "\n",
    "As we have to run the notebooks of all students, and because writing efficient code is important, **your notebook should run in 5 minutes at most**, on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2018/assignment4/problem/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to. After you placed it there, **rename the file** to your UCPH ID (of the form `xxxxxx`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit these**. \n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit these**. \n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "**Do not share** this assignment publicly, by uploading it online, emailing it to friends etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries and Data\n",
    "This cell loads libraries and datasets important for evaluation and assessment of your model. **Do not change it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 1\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "from collections import defaultdict\n",
    "import math\n",
    "import statnlpbook.util as util\n",
    "import statnlpbook.sequence as seq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train = seq.load_tweebank(_snlp_book_dir + \"data/oct27.splits/oct27.train\")\n",
    "dev = seq.load_tweebank(_snlp_book_dir + \"data/oct27.splits/oct27.dev\")\n",
    "test = seq.load_tweebank(_snlp_book_dir + \"data/oct27.splits/oct27.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Setup the MEMM\n",
    "\n",
    "This cell sets up the Maximum Entropy Markov Model (MEMM) for part-of-speech tagging on the Tweebank dataset.  It defines a simple feature function, a utility function that is needed later, and instantiates the actual `MEMMSequenceLabeler` in the variable `memm`.  **Do not edit this setup section either**.  The task below does not require you to make any changes to the functions or variables defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 2\n",
    "def memm_feat(x,i,hist):\n",
    "    return {\n",
    "        'word:' + str(x[i]): 1.0,\n",
    "        'first_at:' + str(x[i][0:1] == '@'): 1.0,\n",
    "        'is_lower:' + str(x[i].islower()): 1.0,\n",
    "        'last_3' + \"\".join(x[i][-3:]): 1.0,\n",
    "        'last_2' + \"\".join(x[i][-2:]): 1.0,\n",
    "        'prev_y': hist[0],\n",
    "    }\n",
    "\n",
    "def batch_predict(data, beam_predictor):\n",
    "    return [beam_predictor(x)[0][0][0] for x,y in data]\n",
    "\n",
    "memm = seq.MEMMSequenceLabeler(memm_feat, train, order=1, C=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Extend the Viterbi algorithms to *n*-grams\n",
    "\n",
    "This is the core part of the assignment.  You are to take the existing implementation of the Viterbi algorithm below, which conditions only on the previously predicted label, and extend it to condition on the previous *n* labels instead.\n",
    "\n",
    "For example, for the label sequence \"O V N\", the states of the Viterbi algorithm with $n=1$ are `[\"O\", \"V\", \"N\"]`.  With $n=2$, the states of the Viterbi algorithm would be `[(\"PAD\", \"O\"), (\"O\", \"V\"), (\"V\", \"N\")]`.\n",
    "\n",
    "Concretely, you need to modify the `memm_viterbi_search` function below to utilize its newly added `n` argument, which indicates the label n-gram length the algorithm should condition on.  The implementation you see below *does not yet do anything with this argument;* instead, it always conditions on a single label only (`n=1`).  You need to understand this implementation, find the locations in the code that need to be modified, and make the necessary modifications.  To do this, you will also need to change a utility function that is called by `memm_viterbi_search`, and therefore also given below.\n",
    "\n",
    "At the end, the cell below evaluates the Viterbi algorithm using unigrams, bigrams, and trigrams.  Currently, these all return identical accuracy scores, but if your implementation is working correctly, they should get progressively better with $n=2$ and $n=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1: 0.8140161725067385\n",
      "n=2: 0.8140161725067385\n",
      "n=3: 0.8140161725067385\n"
     ]
    }
   ],
   "source": [
    "# TODO: extend this function to condition on tag n-grams, based on its \"n\" argument\n",
    "# nb: ignore the \"width\" argument of this function; it is not important for the purposes of this task\n",
    "def memm_viterbi_search(memm, x, width=2, n=1):\n",
    "    labels = memm.labels()\n",
    "    # 'alpha' and 'beta' hold the Viterbi state;\n",
    "    # 'alpha' stores the best accumulated score for a label sequence,\n",
    "    # 'beta'  stores the backpointer to the previous state from which the best score was achieved\n",
    "    # ---For example,\n",
    "    # 'alpha[2][\"N\"]' gives the accumulated (global) score at index 2 of the sequence\n",
    "    #                 when the item at index 2 is labelled as \"N\";\n",
    "    # 'beta[2][\"N\"]'  points to the label at the *previous* index that was used\n",
    "    #                 to achieve this score (since we are conditioning on the previous label!)\n",
    "    alpha = [{}] \n",
    "    beta = [{}] \n",
    "    # ---NB: changing these dicts to condition on n-grams, for example ('PAD', 'N'),\n",
    "    #        instead of just single labels (like 'N') is the crucial part of your assignment\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    # Initialisation\n",
    "    # ---here, we predict the first label of our sequence, using a special \"PAD\" label\n",
    "    #    as our history since we don't have any previous predictions yet\n",
    "    for label_index, label_score in enumerate(memm.predict_scores_hist(x, 0, [\"PAD\"])):\n",
    "        label = labels[label_index]\n",
    "        alpha[0][label] = label_score\n",
    "        beta[0][label] = \"PAD\"\n",
    "    \n",
    "    # Prune --- you can ignore this step for now\n",
    "    seq.prune_alpha_beta(alpha[0], beta[0], width)\n",
    "    \n",
    "    # Recursion\n",
    "    # ---here, we step through each item of the sequence to label it\n",
    "    for i in range(1, len(x)):\n",
    "        alpha.append(defaultdict(lambda: -math.inf))\n",
    "        beta.append({})\n",
    "        # For each possible label at the *previous* index, we predict\n",
    "        # the scores of all possible labels at the *current* index\n",
    "        for p in alpha[i-1].keys():\n",
    "            for label_index, label_score in enumerate(memm.predict_scores_hist(x, i, [p])):\n",
    "                label = labels[label_index]\n",
    "                new_score =  alpha[i-1][p] + label_score\n",
    "                # ...but we only keep predictions if they lead to a higher global score\n",
    "                if new_score > alpha[i][label]:\n",
    "                    alpha[i][label] = new_score\n",
    "                    beta[i][label] = p\n",
    "\n",
    "        # Prune --- you can ignore this step for now\n",
    "        seq.prune_alpha_beta(alpha[i], beta[i], width)\n",
    "\n",
    "    # Finally, we need to reconstruct the best label sequence from 'alpha' and 'beta'\n",
    "    history = convert_alpha_beta_to_history(x, alpha, beta)\n",
    "    return history[-1], history\n",
    "\n",
    "# This utility function takes the 'alpha' and 'beta' from the Viterbi algorithm\n",
    "# and reconstructs the best label sequence(s) from them.\n",
    "# ---NB: This function can be a bit confusing, but you should only need to make minor changes.\n",
    "#        Try looking at/'print'ing the output of this function\n",
    "#          a) with the original Viterbi function\n",
    "#          b) with your modified Viterbi function\n",
    "#        and try to figure out what needs to be changed based on that.  The outputs should\n",
    "#        have the same structure in both cases.\n",
    "def convert_alpha_beta_to_history(x, alpha, beta):\n",
    "    beams = [defaultdict(lambda: [])]\n",
    "    # Here, we first reconstruct all possible label sequences our algorithm has stored\n",
    "    for i in range(0, len(x)):\n",
    "        beam = {}\n",
    "        for l, s in alpha[i].items():\n",
    "            beam[l] = beams[-1][beta[i][l]] + [l]\n",
    "        beams.append(beam)\n",
    "    del beams[0]\n",
    "    history = [[([], 0.0)]]\n",
    "    # Here, we add the scores\n",
    "    for i in range(0, len(x)):\n",
    "        beam = []\n",
    "        for l, s in sorted(alpha[i].items(), key=lambda t: -t[1]):\n",
    "            beam.append((beams[i][l], s))\n",
    "        history.append(beam)\n",
    "    return history\n",
    "\n",
    "for i in range(1, 4):\n",
    "    acc = seq.accuracy(dev, batch_predict(dev, lambda x: memm_viterbi_search(memm, x, width=2, n=i)))\n",
    "    print(f\"n={i}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Accuracy scores of Viterbi search (50 pts)\n",
    "\n",
    "We assess the correctness of your implementation by running it on the dev and test data, using different values for `n`, and checking whether they match the expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1: 0.8140161725067385\n",
      "n=2: 0.8140161725067385\n",
      "n=3: 0.8140161725067385\n",
      "n=1: 0.8137583892617449\n",
      "n=2: 0.8137583892617449\n",
      "n=3: 0.8137583892617449\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    acc = seq.accuracy(dev, batch_predict(dev, lambda x: memm_viterbi_search(memm, x, width=2, n=i)))\n",
    "    print(f\"n={i}: {acc}\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    acc = seq.accuracy(test, batch_predict(test, lambda x: memm_viterbi_search(memm, x, width=2, n=i)))\n",
    "    print(f\"n={i}: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 4: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 4: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe the algorithms\n",
    "\n",
    "We learned about greedy decoding, beam search decoding, and Viterbi decoding.  In this task, you are to describe the functionality of beam search vs. Viterbi decoding with your own words.\n",
    "\n",
    "Describe what happens for each algorithm when it processes the $n$-th token of a sequence.  You should address:\n",
    "\n",
    "1. what probabilities the algorithm calculates;\n",
    "2. what kind of information it stores;\n",
    "3. how it arrives at full predicted label sequence in the end.\n",
    "\n",
    "Here is an example describing the **greedy decoding** algorithm:\n",
    "\n",
    "> When processing the $n$-th token, the algorithm\n",
    ">\n",
    "> + calculates label probabilities conditioned on the label history for tokens $(1,...,n-1)$;\n",
    "> + picks the label with the highest probability as the prediction for $n$.\n",
    ">\n",
    "> The full predicted label sequence is then the sequence of the individual predictions made for each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 4: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 4: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess your description (30 pts)\n",
    "\n",
    "We will mark your description along the following dimensions:\n",
    "\n",
    "+ Clarity (15pts: very clear, 0pts: we can't figure out what you meant)\n",
    "+ Correctness (15pts: you describe the algorithms exactly, 0pts: complete misunderstanding of the algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 2: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 2: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 3</font>: Beam search vs. Viterbi\n",
    "\n",
    "What are the individual advantages and disadvantages of the beam search algorithm vs. the Viterbi algorithm?  When would you use beam search, when Viterbi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above solution is marked with **\n",
    "<!-- ASSESSMENT 4: START_POINTS -->\n",
    "0\n",
    "<!-- ASSESSMENT 4: END_POINTS --> points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 3</font>: Assess your answer (20 pts)\n",
    "\n",
    "We will mark your answer along the following dimensions:\n",
    "\n",
    "+ Clarity (10pts: very clear, 0pts: we can't figure out what you meant)\n",
    "+ Substance (10pts: you provide good arguments, 0pts: you provide no meaningful arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
