{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/java/jdk1.8'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# pipeline 模式\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, ChiSqSelector, IDF, IndexToString, StringIndexer, VectorIndexer\n",
    "\n",
    "os.environ['JAVA_HOME']\n",
    "sc = SparkContext( 'local', 'test')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(sc.stop)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./word.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'asjdklsadasjdas;ldf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"./word.txt\")\n",
    "textFile\n",
    "textFile.first()\n",
    "# textFile.saveAsTextFile('./writeback')  # 保存新文件到writeback路径\n",
    "# logData = sc.textFile(logFile, 2).cache()\n",
    "# numAs = logData.filter(lambda line: 'a' in line).count()\n",
    "# numBs = logData.filter(lambda line: 'b' in line).count()\n",
    "# print('Lines with a: %s, Lines with b: %s' % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=DenseVector([-1.0, 1.5, 1.3]), label=1.0, myProbability=DenseVector([0.0571, 0.9429]), prediction=1.0)\n",
      "Row(features=DenseVector([3.0, 2.0, -0.1]), label=0.0, myProbability=DenseVector([0.9239, 0.0761]), prediction=0.0)\n",
      "Row(features=DenseVector([0.0, 2.2, -1.5]), label=1.0, myProbability=DenseVector([0.1097, 0.8903]), prediction=1.0)\n"
     ]
    }
   ],
   "source": [
    "# 导入向量和模型\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# from pyspark.sql.session import SparkSession\n",
    "\n",
    "# 准备训练数据\n",
    "# Prepare training data from a list of (label, features) tuples\n",
    "# spark = SparkSession(sc)\n",
    "training = spark.createDataFrame([\n",
    "(1.0,Vectors.dense([0.0,1.1,0.1])),\n",
    "(0.0,Vectors.dense([2.0,1.0,-1.0])),\n",
    "(0.0,Vectors.dense([2.0,1.3,1.0])),\n",
    "(1.0,Vectors.dense([0.0,1.2,-0.5]))],[\"label\",\"features\"])\n",
    "\n",
    "# 创建回归实例，这个实例是Estimator\n",
    "# Create a LogisticRegression instance.This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + '\\n')\n",
    "\n",
    "# 使用lr中的参数训练出model\n",
    "# Learn a LogisticRegression model.This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "\n",
    "#查看model1在fit()中使用的参数\n",
    "# print(\"Model 1 was fit using parameters: \")\n",
    "# print(model1.extractParamMap())\n",
    "\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "# 新的参数，合并为两组参数对\n",
    "paramMap2 = {lr.probabilityCol: 'myProbability'}\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "# paramMapCombined\n",
    "\n",
    "# 重新得到model2并拿出来参数瞅瞅\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "# print('Model 2 was fit using parameters: ')\n",
    "# print(model2.extractParamMap())\n",
    "\n",
    "# 准备测试的数据\n",
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "(1.0,Vectors.dense([-1.0,1.5,1.3])),\n",
    "(0.0,Vectors.dense([3.0,2.0,-0.1])),\n",
    "(1.0,Vectors.dense([0.0,2.2,-1.5]))],[\"label\",\"features\"])\n",
    "\n",
    "prediction = model2.transform(test)\n",
    "selected = prediction.select('features', 'label', 'myProbability', 'prediction')\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-605d976ba39d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-605d976ba39d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    0L\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What pipeline is : \n",
      " [Param(parent='Pipeline_37e203d3d025', name='stages', doc='a list of pipeline stages')]\n"
     ]
    }
   ],
   "source": [
    "#准备测试数据\n",
    "# Pipeline 跑\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "(0,\"a b c d e spark\",1.0),\n",
    "(1,\"b d\",0.0),\n",
    "(2,\"spark f g h\",1.0),\n",
    "(3,\"hadoop mapreduce\",0.0)],[\"id\",\"text\",\"label\"])\n",
    "#构建机器学习流水线\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "print('What pipeline is : \\n', pipeline.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练出model 此处霸占太多资源无法跑通！！！！\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "print('What model is :\\n', model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "(4,\"spark i j k\"),\n",
    "(5,\"l m n\"),\n",
    "(6,\"mapreduce spark\"),\n",
    "(7,\"apache hadoop\")],[\"id\",\"text\"])\n",
    "\n",
    "#预测，打印出想要的结果\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\",\"text\",\"prediction\")\n",
    "\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HashingTF in module pyspark.ml.feature:\n",
      "\n",
      "class HashingTF(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.param.shared.HasNumFeatures, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  Maps a sequence of terms to their term frequencies using the hashing trick.\n",
      " |  Currently we use Austin Appleby's MurmurHash 3 algorithm (MurmurHash3_x86_32)\n",
      " |  to calculate the hash code value for the term object.\n",
      " |  Since a simple modulo is used to transform the hash function to a column index,\n",
      " |  it is advisable to use a power of two as the numFeatures parameter;\n",
      " |  otherwise the features will not be mapped evenly to the columns.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n",
      " |  >>> hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\")\n",
      " |  >>> hashingTF.transform(df).head().features\n",
      " |  SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> hashingTF.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
      " |  SparseVector(10, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> params = {hashingTF.numFeatures: 5, hashingTF.outputCol: \"vector\"}\n",
      " |  >>> hashingTF.transform(df, params).head().vector\n",
      " |  SparseVector(5, {0: 1.0, 1: 1.0, 2: 1.0})\n",
      " |  >>> hashingTFPath = temp_path + \"/hashing-tf\"\n",
      " |  >>> hashingTF.save(hashingTFPath)\n",
      " |  >>> loadedHashingTF = HashingTF.load(hashingTFPath)\n",
      " |  >>> loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()\n",
      " |  True\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HashingTF\n",
      " |      pyspark.ml.wrapper.JavaTransformer\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Transformer\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.shared.HasNumFeatures\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, numFeatures=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, numFeatures=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setParams(self, numFeatures=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, numFeatures=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
      " |      Sets params for this HashingTF.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='If...ents rathe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaTransformer:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Transformer:\n",
      " |  \n",
      " |  transform(self, dataset, params=None)\n",
      " |      Transforms the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params.\n",
      " |      :returns: transformed dataset\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasNumFeatures:\n",
      " |  \n",
      " |  getNumFeatures(self)\n",
      " |      Gets the value of numFeatures or its default value.\n",
      " |  \n",
      " |  setNumFeatures(self, value)\n",
      " |      Sets the value of :py:attr:`numFeatures`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasNumFeatures:\n",
      " |  \n",
      " |  numFeatures = Param(parent='undefined', name='numFeatures', doc='numbe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HashingTF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'spark', 'love', 'i', 'i', 'heard', 'and', 'about']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------+---------------------------------------------+\n",
      "|label|sentence                            |words                                        |\n",
      "+-----+------------------------------------+---------------------------------------------+\n",
      "|0    |I heard about Spark and I love Spark|[i, heard, about, spark, and, i, love, spark]|\n",
      "|0    |I wish Java could use case classes  |[i, wish, java, could, use, case, classes]   |\n",
      "|1    |Logistic regression models are neat |[logistic, regression, models, are, neat]    |\n",
      "+-----+------------------------------------+---------------------------------------------+\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|rawFeatures                                                          |\n",
      "+---------------------------------------------------------------------+\n",
      "|(2000,[240,333,1105,1329,1357,1777],[1.0,1.0,2.0,2.0,1.0,1.0])       |\n",
      "|(2000,[213,342,489,495,1329,1809,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(2000,[286,695,1138,1193,1604],[1.0,1.0,1.0,1.0,1.0])                |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                       |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(2000,[240,333,1105,1329,1357,1777],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.5753641449035617,0.6931471805599453,0.6931471805599453])                       |0    |\n",
      "|(2000,[213,342,489,495,1329,1809,1967],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])|0    |\n",
      "|(2000,[286,695,1138,1193,1604],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                               |1    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 特征抽取\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"I heard about Spark and I love Spark\"),\n",
    "    (0, \"I wish Java could use case classes\"),\n",
    "    (1, \"Logistic regression models are neat\")\n",
    "]).toDF(\"label\", \"sentence\")\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# help(tokenizer.transform)\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "# help(wordsData.show)\n",
    "sorted(list(wordsData.select('words').toPandas()['words'])[0], reverse=True)\n",
    "wordsData.show(truncate=False)\n",
    "\n",
    "# 用HashingTF把句子哈希成特征向量，设置哈希表桶数为2000\n",
    "# 分词序列被变换成一个稀疏特征向量，其中每个单词都被散列成了一个不同的索引值，\n",
    "# 特征向量在某一维度上的值即该词汇在文档中出现的次数。\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.select('rawFeatures').show(truncate=False)\n",
    "\n",
    "# 使用IDF对单纯的词频特征向量进行修正\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select('features', 'label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n",
      "+---+-------------------+-------+----------------+\n",
      "| id|           features|clicked|selectedFeatures|\n",
      "+---+-------------------+-------+----------------+\n",
      "|  7|[10.0,3.3,18.0,7.0]|    1.0|          [10.0]|\n",
      "|  8| [0.0,1.0,12.0,5.0]|    0.0|           [0.0]|\n",
      "|  9| [1.0,0.0,15.0,3.1]|    0.0|           [1.0]|\n",
      "+---+-------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 卡方校验\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "result = selector.fit(df).transform(df)\n",
    "result.show()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([10.0, 3.3, 18.0, 7.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 5.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 3.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "result = selector.fit(df).transform(df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "决策树实现参考：\n",
    "https://blog.csdn.net/baidu_41605403/article/details/83006973\n",
    "kaggle参考：\n",
    "https://www.kaggle.com/benhamner/python-data-visualizations/notebook\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
